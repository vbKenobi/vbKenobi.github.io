---
title: "Optimiality of Gradient Descent and NAGD for Convex Functions"
collection: talks
type: "Presentation"
permalink: /talks/2024OptimizationTheory
venue: "UCLA ACM AI Theory Track Workshop #1"
date: 2024-01-22
location: "Los Angeles, California"
---

This was a talk about formalzing supervised learning mathematically, definining how Gradient Desecent, Stochastic Gradient Decesent, and Nestrov's Accelerated Gradient Descent work. We then move onto proving the convergence rate and speed of these methods given the fact that a function is convex and beta-smooth. We then breifly talk about how PGD can be used in adversarial attacks. You can watch this [here](https://www.youtube.com/watch?v=38MYwHQQ4rg&list=PLPO7_kXilXFYUE5XuPoUaycfyH1lxx9U_&index=4).